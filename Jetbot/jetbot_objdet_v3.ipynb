{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "modified-cliff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from jetbot import Robot\n",
    "from jetbot import Camera\n",
    "import cv2\n",
    "print(cv2.__version__)\n",
    "import asyncio\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets.widgets as widgets\n",
    "import traitlets\n",
    "from jetbot import bgr8_to_jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "portable-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = Robot()\n",
    "#camera = Camera.instance(width=1280, height=720)\n",
    "#camera = Camera.instance(width=1024,height=800)\n",
    "#camera = Camera.instance()\n",
    "#net = cv2.dnn.readNet('data/one_sphere/yolov3.weights','data/one_sphere/yolov3.cfg' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frequent-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "increasing-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "numerical-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "indian-court",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvarguscamerasrc ! video/x-raw(memory:NVMM), width=(int)1280, height=(int)720, format=(string)NV12, framerate=(fraction)30/1 ! nvvidconv flip-method=0 ! video/x-raw, width=(int)1280, height=(int)720, format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink\n"
     ]
    }
   ],
   "source": [
    "from jetbot import Robot\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "robot = Robot()\n",
    "import numpy as np\n",
    "import cv2\n",
    "CLASSES = [\"\",\"cube\",\"cylinder\",\"cuboid\"]\n",
    "NET_GRAPH_PATH = 'models/'\n",
    "COLORS = np.random.uniform(0,255,size=(len(CLASSES),3))\n",
    "MODEL12 = 'ssd_mobilenetv1/more_shapes_50k_steps_under_wall_baseboard'\n",
    "\n",
    "\n",
    "SELECTED_MODEL = MODEL12\n",
    "\n",
    "net = cv2.dnn.readNetFromTensorflow(NET_GRAPH_PATH + SELECTED_MODEL + '/frozen_inference_graph.pb', NET_GRAPH_PATH + SELECTED_MODEL +'/final_graph.pbtxt')\n",
    "\n",
    "\n",
    "#net = cv2.dnn.readNetFromTensorflow('data/07_corridor/2vers/frozen_inference_graph.pb', 'data/07_corridor/2vers/final_graph.pbtxt')\n",
    "IMAGE_NAME = 'blurred_4_sigma_gaussian_image_with_no_detections.png'\n",
    "IMAGE_PATH = 'recordings_10_06/' + IMAGE_NAME\n",
    "IMAGE_TO_OUTPUT = 'output_video/' + SELECTED_MODEL + '/' + IMAGE_NAME\n",
    "\n",
    "def gstreamer_pipeline(\n",
    "    capture_width=1280,\n",
    "    capture_height=720,\n",
    "    display_width=1280,\n",
    "    display_height=720,\n",
    "    framerate=30,\n",
    "    flip_method=0,\n",
    "):\n",
    "    return (\n",
    "        \"nvarguscamerasrc ! \"\n",
    "        \"video/x-raw(memory:NVMM), \"\n",
    "        \"width=(int)%d, height=(int)%d, \"\n",
    "        \"format=(string)NV12, framerate=(fraction)%d/1 ! \"\n",
    "        \"nvvidconv flip-method=%d ! \"\n",
    "        \"video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! \"\n",
    "        \"videoconvert ! \"\n",
    "        \"video/x-raw, format=(string)BGR ! appsink\"\n",
    "        % (\n",
    "            capture_width,\n",
    "            capture_height,\n",
    "            framerate,\n",
    "            flip_method,\n",
    "            display_width,\n",
    "            display_height,\n",
    "        )\n",
    "    )\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "image = 0\n",
    "#cap = cv2.VideoCapture(gstreamer_pipeline(flip_method=0), cv2.CAP_GSTREAMER)\n",
    "#cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "print(gstreamer_pipeline(flip_method=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "demographic-category",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image input size: 339, 600, c: 3\n",
      "0.6690592\n",
      "Id: 2\n",
      "cylinder: 66.91%\n",
      "Detected cylinder at X position: 63\n",
      "startX 63\n",
      "w300.0\n",
      "0.5421327\n",
      "Id: 2\n",
      "cylinder: 54.21%\n",
      "Detected cylinder at X position: 269\n",
      "startX 269\n",
      "w300.0\n",
      "0.9751618\n",
      "Id: 3\n",
      "cuboid: 97.52%\n",
      "Detected cuboid at X position: 315\n",
      "startX 315\n",
      "w300.0\n",
      "startX bigger than half image size\n",
      "0.86303073\n",
      "Id: 3\n",
      "cuboid: 86.30%\n",
      "Detected cuboid at X position: 435\n",
      "startX 435\n",
      "w300.0\n",
      "startX bigger than half image size\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf22d8368c043178a5ef147fd5362e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x02X\\x00\\x00\\x01S\\x08\\x02\\x00\\x00\\x00j+\\xe5\\xd5\\x00\\â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "image_trait = widgets.Image(format='png', width=600, height=339)\n",
    "\n",
    "i=0\n",
    "# while cap.isOpened() and i <30:\n",
    "# boolCaptured, gt = cap.read()\n",
    "gt = cv2.imread(IMAGE_PATH)\n",
    "input_image = cv2.cvtColor(gt, cv2.COLOR_RGBA2RGB)\n",
    "h,w,c = input_image.shape\n",
    "print(\"Image input size: \" + str(h) + \", \" + str(w) + \", c: \" + str(c))\n",
    "blob = cv2.dnn.blobFromImage(image=cv2.resize(input_image,(w,h)), scalefactor=1.0, size=(300,300), swapRB=True)\n",
    "net.setInput(blob)\n",
    "detections = net.forward()\n",
    "for j in np.arange(0, detections.shape[2]):\n",
    "    confidence = detections[0,0,j,2]\n",
    "    if confidence > 0.40:\n",
    "        print(confidence)\n",
    "        idx = int(detections[0,0,j,1])\n",
    "        box = detections[0,0,j,3:7] * np.array([w,h,w,h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "        print(\"Id: \" + str(idx))\n",
    "        label = \"{}: {:.2f}%\".format(CLASSES[idx], confidence*100)\n",
    "        print(label)\n",
    "        cv2.rectangle(input_image, (startX,startY), (endX,endY), COLORS[idx],2)\n",
    "        y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "        cv2.putText(input_image, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx],2)\n",
    "        print(\"Detected \" + str(CLASSES[idx]) +  \" at X position: \" + str(startX))\n",
    "        #starting x of the detected object on the right side\n",
    "        print(\"startX \" + str(startX))\n",
    "        print(\"w\" + str(w/2))\n",
    "        if startX > w/2:\n",
    "            print(\"startX bigger than half image size\")\n",
    "            #apply velocity to the left jetbot's joint\n",
    "            robot.right(speed=0.2)\n",
    "            robot.left(speed=0)\n",
    "            robot.stop()\n",
    "        elif startX < w/2:\n",
    "            #apply velocity to the right jetbot's joint\n",
    "            robot.left(speed=0.2)\n",
    "            robot.right(speed=0)\n",
    "            robot.stop()\n",
    "#         else:\n",
    "#             robot.left(speed=0.1)\n",
    "#             robot.right(speed=0) \n",
    "robot.stop()\n",
    "_, encoded_image = cv2.imencode('.png', input_image)\n",
    "bytes_img_obj = encoded_image.tobytes()\n",
    "gt_widget = widgets.Image(value=bytes_img_obj, format=\"png\",width=600, height=339)\n",
    "camera_link = traitlets.dlink((gt_widget,'value'), (image_trait, 'value'))\n",
    "#     clear_output(wait=True)\n",
    "display(image_trait)\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "impossible-soldier",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5cc26c73c0c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cap' is not defined"
     ]
    }
   ],
   "source": [
    "robot.stop()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-crime",
   "metadata": {},
   "source": [
    "Main changes applied so far: changing the layer batch optimazation FusedBatchNormV3 to FusedBatchNorm because of incongruences in tensorflow module version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
