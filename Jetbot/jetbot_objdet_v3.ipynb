{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regional-payroll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from jetbot import Robot\n",
    "from jetbot import Camera\n",
    "import cv2\n",
    "print(cv2.__version__)\n",
    "import asyncio\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets.widgets as widgets\n",
    "import traitlets\n",
    "from jetbot import bgr8_to_jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smoking-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = Robot()\n",
    "#camera = Camera.instance(width=1280, height=720)\n",
    "#camera = Camera.instance(width=1024,height=800)\n",
    "#camera = Camera.instance()\n",
    "#net = cv2.dnn.readNet('data/one_sphere/yolov3.weights','data/one_sphere/yolov3.cfg' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "arranged-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rural-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tamil-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "precise-premiere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvarguscamerasrc ! video/x-raw(memory:NVMM), width=(int)1280, height=(int)720, format=(string)NV12, framerate=(fraction)30/1 ! nvvidconv flip-method=0 ! video/x-raw, width=(int)1280, height=(int)720, format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink\n"
     ]
    }
   ],
   "source": [
    "from jetbot import Robot\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "robot = Robot()\n",
    "import numpy as np\n",
    "import cv2\n",
    "CLASSES = [\"sphere\"]\n",
    "\n",
    "COLORS = np.random.uniform(0,255,size=(len(CLASSES),3))\n",
    "\n",
    "net = cv2.dnn.readNetFromTensorflow('data/fisheye_graphs/frozen_inference_graph.pb', 'data/fisheye_graphs/final_graph.pbtxt')\n",
    "\n",
    "\n",
    "def gstreamer_pipeline(\n",
    "    capture_width=1280,\n",
    "    capture_height=720,\n",
    "    display_width=1280,\n",
    "    display_height=720,\n",
    "    framerate=30,\n",
    "    flip_method=0,\n",
    "):\n",
    "    return (\n",
    "        \"nvarguscamerasrc ! \"\n",
    "        \"video/x-raw(memory:NVMM), \"\n",
    "        \"width=(int)%d, height=(int)%d, \"\n",
    "        \"format=(string)NV12, framerate=(fraction)%d/1 ! \"\n",
    "        \"nvvidconv flip-method=%d ! \"\n",
    "        \"video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! \"\n",
    "        \"videoconvert ! \"\n",
    "        \"video/x-raw, format=(string)BGR ! appsink\"\n",
    "        % (\n",
    "            capture_width,\n",
    "            capture_height,\n",
    "            framerate,\n",
    "            flip_method,\n",
    "            display_width,\n",
    "            display_height,\n",
    "        )\n",
    "    )\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "image = 0\n",
    "cap = cv2.VideoCapture(gstreamer_pipeline(flip_method=0), cv2.CAP_GSTREAMER)\n",
    "cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "print(gstreamer_pipeline(flip_method=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "absent-immunology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af6d378216443db9bfecefa45bf15ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x05\\x00\\x00\\x00\\x02\\xd0\\x08\\x02\\x00\\x00\\x00@\\x1fJ\\x0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "image_trait = widgets.Image(format='png', width=300, height=300)\n",
    "\n",
    "i=0\n",
    "while cap.isOpened() and i <30:\n",
    "    boolCaptured, gt = cap.read()\n",
    "    input_image = cv2.cvtColor(gt, cv2.COLOR_RGBA2RGB)\n",
    "    h,w,c = input_image.shape\n",
    "    print(\"Image input size: \" + str(h) + \", \" + str(w) + \", c: \" + str(c))\n",
    "    blob = cv2.dnn.blobFromImage(image=cv2.resize(input_image,(w,h)), scalefactor=1.0, size=(300,300), swapRB=True)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    for j in np.arange(0, detections.shape[2]):\n",
    "        confidence = detections[0,0,j,2]\n",
    "        if confidence > 0.70:\n",
    "            print(confidence)\n",
    "            idx = int(detections[0,0,j,1])\n",
    "            box = detections[0,0,j,3:7] * np.array([w,h,w,h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            print(\"Id: \" + str(idx))\n",
    "            label = \"{}: {:.2f}%\".format(CLASSES[0], confidence*100)\n",
    "            print(label)\n",
    "            cv2.rectangle(input_image, (startX,startY), (endX,endY), COLORS[0],2)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(input_image, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[0],2)\n",
    "            print(\"Detected \" + str(CLASSES[0]) +  \" at X position: \" + str(startX))\n",
    "            #starting x of the detected object on the right side\n",
    "            print(\"startX \" + str(startX))\n",
    "            print(\"w\" + str(w/2))\n",
    "            if startX > w/2:\n",
    "                print(\"startX bigger than half image size\")\n",
    "                #apply velocity to the left jetbot's joint\n",
    "                robot.right(speed=0.2)\n",
    "                robot.left(speed=0)\n",
    "                robot.stop()\n",
    "            elif startX < w/2:\n",
    "                #apply velocity to the right jetbot's joint\n",
    "                robot.left(speed=0.2)\n",
    "                robot.right(speed=0)\n",
    "                robot.stop()\n",
    "#         else:\n",
    "#             robot.left(speed=0.1)\n",
    "#             robot.right(speed=0) \n",
    "    robot.stop()\n",
    "    _, encoded_image = cv2.imencode('.png', input_image)\n",
    "    bytes_img_obj = encoded_image.tobytes()\n",
    "    gt_widget = widgets.Image(value=bytes_img_obj, format=\"png\",width=300, height=300)\n",
    "    camera_link = traitlets.dlink((gt_widget,'value'), (image_trait, 'value'))\n",
    "    clear_output(wait=True)\n",
    "    display(image_trait)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.stop()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-fireplace",
   "metadata": {},
   "source": [
    "Main changes applied so far: changing the layer batch optimazation FusedBatchNormV3 to FusedBatchNorm because of incongruences in tensorflow module version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
