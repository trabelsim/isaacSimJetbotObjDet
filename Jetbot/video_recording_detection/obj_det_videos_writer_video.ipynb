{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "impaired-exercise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from jetbot import Robot\n",
    "from jetbot import Camera\n",
    "import cv2\n",
    "print(cv2.__version__)\n",
    "import asyncio\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets.widgets as widgets\n",
    "import traitlets\n",
    "from jetbot import bgr8_to_jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "light-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = Robot()\n",
    "#camera = Camera.instance(width=1280, height=720)\n",
    "#camera = Camera.instance(width=1024,height=800)\n",
    "#camera = Camera.instance()\n",
    "#net = cv2.dnn.readNet('data/one_sphere/yolov3.weights','data/one_sphere/yolov3.cfg' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "familiar-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "opposite-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dress-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "polish-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTTING PURPOSE\n",
    "time_r = []\n",
    "detections = []\n",
    "counter = 0\n",
    "indexer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import FileVideoStream, FPS\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from threading import Thread\n",
    "import cv2, time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ThreadedCamera:\n",
    "    def __init__(self, path, path_out):\n",
    "        self.capture = cv2.VideoCapture(path)\n",
    "        self.capture.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "        # FPS = 1/X\n",
    "        # X = desired FPS\n",
    "#         self.FPS = 1/40\n",
    "        self.FPS = 1/ int((self.capture.get(cv2.CAP_PROP_FPS)))\n",
    "        print(\"FPS delay: \" + str(self.FPS))\n",
    "        print(\"Get FPS value: \" + str(int(self.capture.get(cv2.CAP_PROP_FPS))))\n",
    "        self.FPS_MS = int(self.FPS * 1000)\n",
    "        \n",
    "        self.list_of_det = []\n",
    "        self.counter = 0\n",
    "        self.previous_frame = None\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "        #Hardcoded width and height values, as those are being read only once the frame is captured\n",
    "        self.output_video = cv2.VideoWriter(path_out, self.fourcc, int(self.capture.get(cv2.CAP_PROP_FPS)/20), (1280,720))\n",
    "        self.availableFrames = True\n",
    "        self.initialized = False\n",
    "        # Start frame retrieval thread\n",
    "        self.thread = Thread(target=self.update, args=())\n",
    "        self.thread.daemon = True\n",
    "        self.thread.start()\n",
    "\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        while True:\n",
    "            if self.capture.isOpened():\n",
    "                (self.status, self.frame) = self.capture.read()\n",
    "                if self.initialized == False:\n",
    "                    self.previous_frame = self.frame\n",
    "                    self.initialized=True\n",
    "                if self.status == False:\n",
    "                    self.availableFrames = False\n",
    "                    sys.exit()\n",
    "                time.sleep(self.FPS)\n",
    "#                 print(\"self.capture is opened(): \" + str(time.time()))\n",
    "            \n",
    "    def get_frame(self):\n",
    "        global counter, indexer, detections\n",
    "        input_image = cv2.cvtColor(self.frame, cv2.COLOR_RGBA2RGB)\n",
    "        h,w,c = input_image.shape\n",
    "        print(\"Image input size: \" + str(h) + \", \" + str(w) + \", c: \" + str(c))\n",
    "        blob = cv2.dnn.blobFromImage(image=cv2.resize(input_image,(w,h)), scalefactor=1.0, size=(300,300), swapRB=True)\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        for j in np.arange(0, detections.shape[2]):\n",
    "            confidence = detections[0,0,j,2]\n",
    "            if confidence > 0.50:\n",
    "                self.counter = self.counter + 1\n",
    "                idx = int(detections[0,0,j,1])\n",
    "                box = detections[0,0,j,3:7] * np.array([w,h,w,h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                print(\"Id: \" + str(idx))\n",
    "                label = \"{}: {:.2f}%\".format(CLASSES[idx], confidence*100)\n",
    "                print(label)\n",
    "                # Checking if the box detected is not too big -> false positive\n",
    "                if endX - startX < 300:\n",
    "                    cv2.rectangle(input_image, (startX,startY), (endX,endY), COLORS[idx],2)\n",
    "                    y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "                    cv2.putText(input_image, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx],2)\n",
    "                    print(\"Detected \" + str(CLASSES[idx]) +  \" at X position: \" + str(startX))\n",
    "                    #starting x of the detected object on the right side\n",
    "                    print(\"startX \" + str(startX))\n",
    "                    print(\"w\" + str(w/2))\n",
    "#         difference = cv2.subtract(self.previous_frame, input_image)\n",
    "#         if np.any(difference) and self.initialized:\n",
    "        self.output_video.write(input_image)\n",
    "        self.list_of_det.append(self.counter)\n",
    "        self.previous_frame = input_image\n",
    "        self.counter = 0\n",
    "#         _, encoded_image = cv2.imencode('.png', input_image)\n",
    "#         bytes_img_obj = encoded_image.tobytes()\n",
    "#         gt_widget = widgets.Image(value=bytes_img_obj, format=\"png\",width=300, height=300)\n",
    "#         camera_link = traitlets.dlink((gt_widget,'value'), (image_trait, 'value'))\n",
    "#         if(self.initialized == False):\n",
    "#     #         clear_output(wait=True)\n",
    "#             display(image_trait)\n",
    "#             self.initialized = True\n",
    "#         print(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "robot = Robot()\n",
    "import numpy as np\n",
    "import cv2\n",
    "CLASSES = [\"cube\",\"cylinder\",\"cuboid\"]\n",
    "\n",
    "COLORS = np.random.uniform(0,255,size=(len(CLASSES),3))\n",
    "\n",
    "NET_GRAPH_PATH = 'models/'\n",
    "\n",
    "\n",
    "# SSD_MOBILENETV1 trained on basic scene for 50k steps\n",
    "MODEL1 = 'ssd_mobilenetv1/basic_50k_steps'\n",
    "# SSD_MOBILENETV1 trained on basic scene for 100k steps\n",
    "MODEL2 = 'ssd_mobilenetv1/basic_100k_steps'\n",
    "# SSD_MOBILENETV2 trained on basic scene for 50k steps\n",
    "MODEL3 = 'ssd_mobilenetv2/basic_50k_steps'\n",
    "# SSD_INCEPTIONV2 trained on basic scene for 50k steps\n",
    "MODEL4 = 'ssd_inceptionv2/basic_50k_steps'\n",
    "# SSD_MOBILENETV1 trained on darker scenes (light randomization) for 50k steps\n",
    "MODEL5 = 'ssd_mobilenetv1/dark_light_50k_steps'\n",
    "# SSD_MOBILENETV1 trained on basic scene for 50k steps with additional background images (5%)\n",
    "MODEL6 = 'ssd_mobilenetv1/basic_50k_steps_add_backgr_img'\n",
    "# SSD_MOBILENETV1 trained on basic scene for 100k steps with additional background images (5%)\n",
    "MODEL7 = 'ssd_mobilenetv1/basic_100k_steps_add_backgr_img'\n",
    "# MODEL5 = 'ssd_mobilenetv1/ground_text'\n",
    "# MODEL6 = 'ssd_mobilenetv1/multi_ground_text'\n",
    "# MODEL7 = 'ssd_mobilenetv1/white_floor_tile'\n",
    "\n",
    "# not good results\n",
    "MODEL8 = 'ssd_mobilenetv1/basic_50k_steps_add_backgr_img_2k'\n",
    "\n",
    "# SSD_MOBILENETV1 trained on an extended scene with different sizes of objects for\n",
    "# 50k steps and with additional background images (5%)\n",
    "MODEL9 = 'ssd_mobilenetv1/more_shapes_50k_steps_add_backgr'\n",
    "\n",
    "# SSD_MOBILENETV1 trained on an extended scene with different sizes of objects for\n",
    "# 50k steps, with additional background images (5%) and for multiple instances\n",
    "MODEL10 = 'ssd_mobilenetv1/more_shapes_50k_steps_add_backgr_more_instances'\n",
    "\n",
    "MODEL11 = 'ssd_mobilenetv1/more_shapes_50k_steps_under_wall'\n",
    "\n",
    "MODEL12 = 'ssd_mobilenetv1/more_shapes_50k_steps_under_wall_baseboard'\n",
    "\n",
    "# ########\n",
    "# MODEL11 = 'ssd_mobilenetv1/2k_images_data/100k'\n",
    "SELECTED_MODEL = MODEL12\n",
    "\n",
    "net = cv2.dnn.readNetFromTensorflow(NET_GRAPH_PATH + SELECTED_MODEL + '/frozen_inference_graph.pb', NET_GRAPH_PATH + SELECTED_MODEL +'/final_graph.pbtxt')\n",
    "\n",
    "VIDEO_NAME = 'drive_1_1'\n",
    "VIDEO_TO_PROCESS = 'recordings_10_06/' + VIDEO_NAME + '.mp4'\n",
    "VIDEO_TO_OUTPUT = 'output_video/' + SELECTED_MODEL + '/' +VIDEO_NAME + '.avi'\n",
    "\n",
    "IMAGE_NAME = 'img_9640.png'\n",
    "IMAGE_TO_OUTPUT = 'output_video/' + SELECTED_MODEL + '/' + IMAGE_NAME\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "image = 0\n",
    "# cap = cv2.VideoCapture(VIDEO_TO_PROCESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "fvs = ThreadedCamera(VIDEO_TO_PROCESS, VIDEO_TO_OUTPUT)\n",
    "\n",
    "image_trait = widgets.Image(format='png', width=300, height=300)\n",
    "\n",
    "i=0\n",
    "fps = FPS().start()\n",
    "while fvs.availableFrames:\n",
    "    try:\n",
    "        fvs.get_frame()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    except AssertionError:\n",
    "        pass\n",
    "fps.stop()\n",
    "print(\"INFO: Elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"INFO: Approx FPS: {:.2f}\".format(fps.fps()))\n",
    "# fvs.stop()\n",
    "import matplotlib.pyplot as plt\n",
    "time_r = range(len(fvs.list_of_det))\n",
    "plt.plot(time_r,fvs.list_of_det)\n",
    "plt.xlabel('num. of processed frames')\n",
    "plt.ylabel('num. of detections')\n",
    "plt.show()\n",
    "# PLT_TO_OUTPUT = NET_GRAPH_PATH + SELECTED_MODEL + '/output_video/' +VIDEO_NAME + 'PlotResult.jpg'\n",
    "plt.savefig(PLT_TO_OUTPUT)\n",
    "fvs.capture.release()\n",
    "fvs.output_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-mambo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-tours",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "attractive-feelings",
   "metadata": {},
   "source": [
    "Main changes applied so far: changing the layer batch optimazation FusedBatchNormV3 to FusedBatchNorm because of incongruences in tensorflow module version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
